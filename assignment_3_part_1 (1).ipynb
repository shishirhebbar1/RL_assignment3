{
  "cells": [
    {
      "metadata": {
        "id": "cf4d8c9b006ada45"
      },
      "cell_type": "markdown",
      "source": [
        "## <center>CSE 546: Reinforcement Learning</center>\n",
        "### <center>Prof. Alina Vereshchaka</center>\n",
        "#### <center>Spring 2025</center>\n",
        "\n",
        "Welcome to the Assignment 3, Part 1: Introduction to Actor-Critic Methods! It includes the implementation of simple actor and critic networks and best practices used in modern Actor-Critic algorithms."
      ],
      "id": "cf4d8c9b006ada45"
    },
    {
      "metadata": {
        "id": "9d7a6d891e2fb312"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 0: Setup and Imports"
      ],
      "id": "9d7a6d891e2fb312"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53473293aa9daf8e",
        "outputId": "41e6d97d-4327-4bd7-edcb-fbbb808f7120"
      },
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\"\n",
        "!pip install gymnasium[atari,accept-rom-license]\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils as utils\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "id": "53473293aa9daf8e",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "\u001b[33mWARNING: gymnasium 1.1.1 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (0.10.2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7852900f4030>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "execution_count": 23
    },
    {
      "metadata": {
        "id": "2a3d9c34ff222994"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 1: Actor-Critic Network Architectures and Loss Computation\n",
        "\n",
        "In this section, you will explore two common architectural designs for Actor-Critic methods and implement their corresponding loss functions using dummy tensors. These architectures are:\n",
        "- A. Completely separate actor and critic networks\n",
        "- B. A shared network with two output heads\n",
        "\n",
        "Both designs are widely used in practice. Shared networks are often more efficient and generalize better, while separate networks offer more control and flexibility.\n",
        "\n",
        "---\n"
      ],
      "id": "2a3d9c34ff222994"
    },
    {
      "metadata": {
        "id": "971fa7887dd4f858"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 1a ‚Äì Separate Actor and Critic Networks with Loss Function\n",
        "\n",
        "Define a class `SeparateActorCritic`. Your goal is to:\n",
        "- Create two completely independent neural networks: one for the actor and one for the critic.\n",
        "- The actor should output a probability distribution over discrete actions (use `nn.Softmax`).\n",
        "- The critic should output a single scalar value.\n",
        "\n",
        " Use `nn.ReLU()` as your activation function. Include at least one hidden layer of reasonable width (e.g. 64 or 128 units).\n",
        "\n",
        "```python\n",
        "# TODO: Define SeparateActorCritic class\n",
        "```\n",
        "\n",
        " Next, simulate training using dummy tensors:\n",
        "1. Generate dummy tensors for log-probabilities, returns, estimated values, and entropies.\n",
        "2. Compute the actor loss using the advantage (return - value).\n",
        "3. Compute the critic loss as mean squared error between values and returns.\n",
        "4. Use a single optimizer for both the Actor and the Critic. In this case, combine the actor and critic losses into a total loss and perform backpropagation.\n",
        "5. Use a separate optimizers for both the Actor and the Critic. In this case, keep the actor and critic losses separate and perform backpropagation.\n",
        "\n",
        "```python\n",
        "# TODO: Simulate loss computation and backpropagation\n",
        "```\n",
        "\n",
        "üîó Helpful references:\n",
        "- PyTorch Softmax: https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
        "- PyTorch MSE Loss: https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html\n",
        "\n",
        "---"
      ],
      "id": "971fa7887dd4f858"
    },
    {
      "metadata": {
        "id": "dd6b81ed1791e4e6"
      },
      "cell_type": "code",
      "source": [
        "# TODO: Define a class SeparateActorCritic with separate networks for actor and critic\n",
        "\n",
        "# BEGIN_YOUR_CODE\n",
        "class SeparateActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state_dim (int): Dimension of state/input.\n",
        "            action_dim (int): Number of discrete actions.\n",
        "            hidden_size (int): Width of the hidden layer.\n",
        "        \"\"\"\n",
        "        super(SeparateActorCritic, self).__init__()\n",
        "        # Actor network\n",
        "        self.actor_fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.actor_fc2 = nn.Linear(hidden_size, action_dim)\n",
        "        self.actor_softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        # Critic network\n",
        "        self.critic_fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.critic_fc2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Forward pass for both actor and critic.\"\"\"\n",
        "        # Actor forward pass\n",
        "        actor_hidden = F.relu(self.actor_fc1(state))\n",
        "        action_logits = self.actor_fc2(actor_hidden)\n",
        "        action_probs = self.actor_softmax(action_logits)\n",
        "\n",
        "        # Critic forward pass\n",
        "        critic_hidden = F.relu(self.critic_fc1(state))\n",
        "        value = self.critic_fc2(critic_hidden)\n",
        "\n",
        "        return action_probs, value\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "dd6b81ed1791e4e6",
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_training_single_optimizer(model, state):\n",
        "    \"\"\"\n",
        "    Simulate training using dummy tensors with a single optimizer for both Actor and Critic.\n",
        "    \"\"\"\n",
        "    # Forward pass through our model\n",
        "    action_probs, value = model(state)\n",
        "\n",
        "    # --- Dummy tensor simulation ---\n",
        "    # Simulate dummy log probabilities from the actor; here we assume a batch of actions\n",
        "    dummy_log_probs = torch.log(action_probs + 1e-8)\n",
        "\n",
        "    # Simulated returns (e.g., discounted rewards)\n",
        "    dummy_returns = torch.randn_like(value)\n",
        "\n",
        "    # Simulated dummy entropies (for the actor)\n",
        "    dummy_entropies = -torch.sum(action_probs * dummy_log_probs, dim=1, keepdim=True)\n",
        "\n",
        "    # Compute advantage: (return - value)\n",
        "    advantage = dummy_returns - value.detach()\n",
        "\n",
        "    # Actor loss: negative log probability * advantage - entropy bonus (we use a factor of 0.01 for entropy regularization)\n",
        "    actor_loss = -(dummy_log_probs * advantage).mean() - 0.01 * dummy_entropies.mean()\n",
        "\n",
        "    # Critic loss: mean squared error between value and return\n",
        "    critic_loss = F.mse_loss(value, dummy_returns)\n",
        "\n",
        "    # Total loss as weighted sum (here we assume equal weighting)\n",
        "    total_loss = actor_loss + critic_loss\n",
        "\n",
        "    # Single optimizer: update model parameters\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(\"Single Optimizer:\")\n",
        "    print(\"Actor Loss:\", actor_loss.item())\n",
        "    print(\"Critic Loss:\", critic_loss.item())\n",
        "    print(\"Total Loss:\", total_loss.item())\n",
        "\n",
        "\n",
        "def simulate_training_separate_optimizers(model, state):\n",
        "    \"\"\"\n",
        "    Simulate training using dummy tensors with separate optimizers for Actor and Critic.\n",
        "    \"\"\"\n",
        "    # Forward pass through our model\n",
        "    action_probs, value = model(state)\n",
        "\n",
        "    # --- Dummy tensor simulation ---\n",
        "    dummy_log_probs = torch.log(action_probs + 1e-8)\n",
        "    dummy_returns = torch.randn_like(value)\n",
        "    dummy_entropies = -torch.sum(action_probs * dummy_log_probs, dim=1, keepdim=True)\n",
        "    advantage = dummy_returns - value.detach()\n",
        "\n",
        "    actor_loss = -(dummy_log_probs * advantage).mean() - 0.01 * dummy_entropies.mean()\n",
        "    critic_loss = F.mse_loss(value, dummy_returns)\n",
        "\n",
        "    # Separate optimizers for actor and critic\n",
        "    # We need to separate the parameters that belong to each network.\n",
        "    actor_params = list(model.actor_fc1.parameters()) + list(model.actor_fc2.parameters())\n",
        "    critic_params = list(model.critic_fc1.parameters()) + list(model.critic_fc2.parameters())\n",
        "\n",
        "    optimizer_actor = optim.Adam(actor_params, lr=1e-3)\n",
        "    optimizer_critic = optim.Adam(critic_params, lr=1e-3)\n",
        "\n",
        "    # Update actor parameters\n",
        "    optimizer_actor.zero_grad()\n",
        "    actor_loss.backward(retain_graph=True)  # Retain graph because critic loss still needs the graph.\n",
        "    optimizer_actor.step()\n",
        "\n",
        "    # Update critic parameters\n",
        "    optimizer_critic.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    optimizer_critic.step()\n",
        "\n",
        "    print(\"\\nSeparate Optimizers:\")\n",
        "    print(\"Actor Loss:\", actor_loss.item())\n",
        "    print(\"Critic Loss:\", critic_loss.item())"
      ],
      "metadata": {
        "id": "-zYQo-UgYdyO"
      },
      "id": "-zYQo-UgYdyO",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dim = 10\n",
        "action_dim = 4\n",
        "batch_size = 5  # simulate a batch of 5 states\n",
        "\n",
        "# Create dummy state tensor\n",
        "dummy_state = torch.randn(batch_size, state_dim)\n",
        "\n",
        "# Instantiate the model\n",
        "model = SeparateActorCritic(state_dim, action_dim, hidden_size=64)\n",
        "\n",
        "# Simulate training with a single optimizer for both networks.\n",
        "simulate_training_single_optimizer(model, dummy_state)\n",
        "\n",
        "# Simulate training with separate optimizers for actor and critic.\n",
        "simulate_training_separate_optimizers(model, dummy_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwIofiUFYnWG",
        "outputId": "b03e0a15-e850-4fed-8e7a-dc01441c922e"
      },
      "id": "gwIofiUFYnWG",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Optimizer:\n",
            "Actor Loss: 0.2998315095901489\n",
            "Critic Loss: 2.5514073371887207\n",
            "Total Loss: 2.85123872756958\n",
            "\n",
            "Separate Optimizers:\n",
            "Actor Loss: -0.46809422969818115\n",
            "Critic Loss: 1.2666232585906982\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "eb8e90c88108cd2e"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "eb8e90c88108cd2e"
    },
    {
      "metadata": {
        "id": "98ea382314354335"
      },
      "cell_type": "code",
      "source": [],
      "id": "98ea382314354335",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "64081a606b93029d"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 1b ‚Äì Shared Network with Actor and Critic Heads + Loss Function\n",
        "\n",
        "Now define a class `SharedActorCritic`:\n",
        "- Build a shared base network (e.g., linear layer + ReLU)\n",
        "- Create two heads: one for actor (output action probabilities) and one for critic (output state value)\n",
        "\n",
        "```python\n",
        "# TODO: Define SharedActorCritic class\n",
        "```\n",
        "\n",
        "Then:\n",
        "1. Pass a dummy input tensor through the model to obtain action probabilities and value.\n",
        "2. Simulate dummy rewards and compute advantage.\n",
        "3. Compute the actor and critic losses, combine them, and backpropagate.\n",
        "\n",
        "```python\n",
        "# TODO: Simulate shared network loss computation and backpropagation\n",
        "```\n",
        "\n",
        " Use `nn.Softmax` for actor output and `nn.Linear` for scalar critic output.\n",
        "\n",
        "üîó More reading:\n",
        "- Policy Gradient Methods: https://spinningup.openai.com/en/latest/algorithms/vpg.html\n",
        "- Actor-Critic Overview: https://www.tensorflow.org/agents/tutorials/6_reinforce_tutorial\n",
        "- PyTorch Categorical Distribution: https://pytorch.org/docs/stable/distributions.html#categorical\n",
        "\n",
        "---"
      ],
      "id": "64081a606b93029d"
    },
    {
      "metadata": {
        "id": "a48f882fff11aecc"
      },
      "cell_type": "code",
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "class SharedActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state_dim (int): Dimension of state/input.\n",
        "            action_dim (int): Number of discrete actions.\n",
        "            hidden_size (int): Number of hidden units for the shared network.\n",
        "        \"\"\"\n",
        "        super(SharedActorCritic, self).__init__()\n",
        "\n",
        "        # Shared base network (e.g., one hidden layer)\n",
        "        self.shared_fc = nn.Linear(state_dim, hidden_size)\n",
        "\n",
        "        # Actor head: outputs action logits that will be converted to probabilities\n",
        "        self.actor_fc = nn.Linear(hidden_size, action_dim)\n",
        "        self.actor_softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        # Critic head: outputs a single scalar value for the state\n",
        "        self.critic_fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Forward pass for the shared network.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor): Input tensor representing the state.\n",
        "\n",
        "        Returns:\n",
        "            action_probs (torch.Tensor): Probability distribution over actions.\n",
        "            value (torch.Tensor): Scalar state value.\n",
        "        \"\"\"\n",
        "        # Shared network\n",
        "        shared_hidden = F.relu(self.shared_fc(state))\n",
        "\n",
        "        # Actor branch: produce action probabilities\n",
        "        action_logits = self.actor_fc(shared_hidden)\n",
        "        action_probs = self.actor_softmax(action_logits)\n",
        "\n",
        "        # Critic branch: produce state value\n",
        "        value = self.critic_fc(shared_hidden)\n",
        "\n",
        "        return action_probs, value\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "a48f882fff11aecc",
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_training_shared_network(model, state):\n",
        "    \"\"\"\n",
        "    Simulate training on the shared network using a dummy input.\n",
        "    The procedure includes:\n",
        "    - Forward pass through model to obtain action probabilities and value.\n",
        "    - Dummy rewards and advantage computation.\n",
        "    - Loss computation for both actor and critic.\n",
        "    - Combined loss backpropagation using a single optimizer.\n",
        "    \"\"\"\n",
        "    # Forward pass through the shared network\n",
        "    action_probs, value = model(state)\n",
        "\n",
        "    # --- Dummy data simulation ---\n",
        "    # For actor loss: generate dummy log probabilities from the actor\n",
        "    dummy_log_probs = torch.log(action_probs + 1e-8)\n",
        "\n",
        "    # Simulate dummy rewards or returns\n",
        "    dummy_returns = torch.randn_like(value)\n",
        "\n",
        "    # Compute advantage (return - value)\n",
        "    advantage = dummy_returns - value.detach()\n",
        "\n",
        "    # Compute an entropy bonus (to encourage exploration)\n",
        "    dummy_entropies = -torch.sum(action_probs * dummy_log_probs, dim=1, keepdim=True)\n",
        "\n",
        "    # Actor loss: negative log probability weighted by advantage, with an entropy bonus.\n",
        "    actor_loss = -(dummy_log_probs * advantage).mean() - 0.01 * dummy_entropies.mean()\n",
        "\n",
        "    # Critic loss: mean squared error (MSE) between estimated value and return.\n",
        "    critic_loss = F.mse_loss(value, dummy_returns)\n",
        "\n",
        "    # Combine losses (here we simply sum them)\n",
        "    total_loss = actor_loss + critic_loss\n",
        "\n",
        "    # Backpropagation using a single optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print losses for reference\n",
        "    print(\"Shared Network Training:\")\n",
        "    print(\"Actor Loss:\", actor_loss.item())\n",
        "    print(\"Critic Loss:\", critic_loss.item())\n",
        "    print(\"Total Loss:\", total_loss.item())"
      ],
      "metadata": {
        "id": "BWcfMCwzZeMG"
      },
      "id": "BWcfMCwzZeMG",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dim = 10    # Dimension of the state\n",
        "action_dim = 4    # Number of discrete actions\n",
        "batch_size = 5    # Batch size for dummy input\n",
        "\n",
        "# Create a dummy state tensor with shape (batch_size, state_dim)\n",
        "dummy_state = torch.randn(batch_size, state_dim)\n",
        "\n",
        "# Initialize the shared actor-critic model\n",
        "model = SharedActorCritic(state_dim, action_dim, hidden_size=64)\n",
        "\n",
        "# Simulate training using the shared network\n",
        "simulate_training_shared_network(model, dummy_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lNYmqzHZhtR",
        "outputId": "0f9af1d5-2767-4a67-f724-976a74de5385"
      },
      "id": "9lNYmqzHZhtR",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shared Network Training:\n",
            "Actor Loss: -0.031828150153160095\n",
            "Critic Loss: 0.6814764142036438\n",
            "Total Loss: 0.6496482491493225\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "a974e302d1fdb028"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "a974e302d1fdb028"
    },
    {
      "metadata": {
        "id": "8fad5ea9406f8b4b"
      },
      "cell_type": "code",
      "source": [],
      "id": "8fad5ea9406f8b4b",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "eb645eb009b85b1c"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 2: Auto-Adaptive Network Setup for Environments\n",
        "\n",
        "You will now create a function that builds a shared actor-critic network that adapts to any Gymnasium environment. This function should inspect the environment and build input/output layers accordingly."
      ],
      "id": "eb645eb009b85b1c"
    },
    {
      "metadata": {
        "id": "4223b6ddf43abee5"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 2: Auto-generate Input and Output Layers\n",
        "Write a function `create_shared_network(env)` that constructs a neural network using the following rules:\n",
        "- The input layer should match the environment's observation space.\n",
        "- The output layer for the **actor** should depend on the action space:\n",
        "  - For discrete actions: output probabilities using `nn.Softmax`.\n",
        "  - For continuous actions: output mean and log std for a Gaussian distribution.\n",
        "- The **critic** always outputs a single scalar value.\n",
        "\n",
        "```python\n",
        "# TODO: Define function `create_shared_network(env)`\n",
        "```\n",
        "\n",
        "#### Environments to Support:\n",
        "Test your function with the following environments:\n",
        "1. `CliffWalking-v0` (Use one-hot encoding for discrete integer observations.)\n",
        "2. `LunarLander-v3` (Standard Box space for observations and discrete actions.)\n",
        "3. `PongNoFrameskip-v4` (Use gym wrappers for Atari image preprocessing.)\n",
        "4. `HalfCheetah-v5` (Continuous observation and continuous action.)\n",
        "\n",
        "```python\n",
        "# TODO: Loop through environments and test `create_shared_network`\n",
        "```\n",
        "\n",
        "Hint: Use `gym.spaces` utilities to determine observation/action types dynamically.\n",
        "\n",
        "üîó Observation/Action Space Docs:\n",
        "- https://gymnasium.farama.org/api/spaces/\n",
        "\n",
        "---"
      ],
      "id": "4223b6ddf43abee5"
    },
    {
      "metadata": {
        "id": "d6d249ff9277403a"
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Shared Actor-Critic Network\n",
        "class ActorCriticNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_size=128, is_continuous=False):\n",
        "        super(ActorCriticNet, self).__init__()\n",
        "        self.is_continuous = is_continuous\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
        "        self.actor = nn.Linear(hidden_size, output_dim if not is_continuous else hidden_size)\n",
        "        self.critic = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        if is_continuous:\n",
        "            self.log_std = nn.Parameter(torch.zeros(output_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        value = self.critic(x)\n",
        "\n",
        "        if self.is_continuous:\n",
        "            mean = self.actor(x)\n",
        "            return (mean, self.log_std.exp()), value\n",
        "        else:\n",
        "            logits = self.actor(x)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            return probs, value"
      ],
      "id": "d6d249ff9277403a",
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "def create_shared_network(env, hidden_size=128):\n",
        "    obs_space = env.observation_space\n",
        "    act_space = env.action_space\n",
        "\n",
        "    if isinstance(obs_space, gym.spaces.Discrete):\n",
        "        input_dim = obs_space.n\n",
        "    else:  # Box\n",
        "        input_dim = int(torch.prod(torch.tensor(obs_space.shape)))\n",
        "\n",
        "    is_continuous = isinstance(act_space, gym.spaces.Box)\n",
        "    output_dim = act_space.shape[0] if is_continuous else act_space.n\n",
        "\n",
        "    return ActorCriticNet(input_dim, output_dim, hidden_size, is_continuous)\n"
      ],
      "metadata": {
        "id": "L6zW1URUa_U4"
      },
      "id": "L6zW1URUa_U4",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_names = [\n",
        "    \"CliffWalking-v0\",       # Discrete observation, discrete action\n",
        "    \"LunarLander-v2\",        # Box observation, discrete action\n",
        "    \"PongNoFrameskip-v4\",    # Atari env, pixel-based\n",
        "    \"HalfCheetah-v2\",        # Continuous observation and action\n",
        "]\n",
        "\n",
        "for env_name in env_names:\n",
        "    try:\n",
        "        env = gym.make(env_name)\n",
        "        print(f\"\\nCreating network for {env_name}:\")\n",
        "        model = create_shared_network(env, hidden_size=128)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create environment {env_name}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Generate dummy input\n",
        "    if isinstance(env.observation_space, gym.spaces.Discrete):\n",
        "        dummy_input = torch.nn.functional.one_hot(torch.tensor([0, 1, 2]), num_classes=env.observation_space.n).float()\n",
        "    elif isinstance(env.observation_space, gym.spaces.Box):\n",
        "        shape = env.observation_space.shape\n",
        "        input_dim = int(torch.prod(torch.tensor(shape)))\n",
        "        dummy_input = torch.randn(3, input_dim)\n",
        "    else:\n",
        "        print(\"Unsupported observation space.\")\n",
        "        continue\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(dummy_input)\n",
        "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
        "        action_probs, value = output\n",
        "        print(\"Actor Output (Action Probabilities):\", action_probs)\n",
        "        print(\"Critic Output (State Value):\", value)\n",
        "    elif isinstance(env.action_space, gym.spaces.Box):\n",
        "        (mean, std), value = output\n",
        "        print(\"Actor Output (Mean):\", mean)\n",
        "        print(\"Actor Output (Std):\", std)\n",
        "        print(\"Critic Output (State Value):\", value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iSzCbarbDq3",
        "outputId": "f83238c2-99c8-46cf-b15d-688e0484bc2e"
      },
      "id": "9iSzCbarbDq3",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating network for CliffWalking-v0:\n",
            "Actor Output (Action Probabilities): tensor([[0.2537, 0.2514, 0.2460, 0.2489],\n",
            "        [0.2448, 0.2540, 0.2606, 0.2406],\n",
            "        [0.2377, 0.2675, 0.2461, 0.2487]], grad_fn=<SoftmaxBackward0>)\n",
            "Critic Output (State Value): tensor([[0.1194],\n",
            "        [0.0913],\n",
            "        [0.1154]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Creating network for LunarLander-v2:\n",
            "Actor Output (Action Probabilities): tensor([[0.1882, 0.2264, 0.2945, 0.2909],\n",
            "        [0.1614, 0.1792, 0.3211, 0.3382],\n",
            "        [0.2363, 0.1976, 0.2451, 0.3210]], grad_fn=<SoftmaxBackward0>)\n",
            "Critic Output (State Value): tensor([[ 0.0307],\n",
            "        [ 0.0077],\n",
            "        [-0.3122]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Creating network for PongNoFrameskip-v4:\n",
            "Actor Output (Action Probabilities): tensor([[0.2295, 0.1743, 0.1103, 0.1654, 0.1799, 0.1406],\n",
            "        [0.1752, 0.1676, 0.1420, 0.1350, 0.2967, 0.0835],\n",
            "        [0.1989, 0.1761, 0.1338, 0.1334, 0.1816, 0.1763]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Critic Output (State Value): tensor([[0.3760],\n",
            "        [0.5028],\n",
            "        [0.3023]], grad_fn=<AddmmBackward0>)\n",
            "Failed to create environment HalfCheetah-v2: No module named 'mujoco_py'. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment HalfCheetah-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"gymnasium[mujoco]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EcvxyyJrVae",
        "outputId": "1bc87cbf-704d-4945-ed53-6a17493c4ece"
      },
      "id": "0EcvxyyJrVae",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.2)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[mujoco])\n",
            "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.21.0)\n",
            "Downloading mujoco-3.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.8.0 mujoco-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Shared network definition for both discrete and continuous action spaces\n",
        "class SharedNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, action_space, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.critic = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        if isinstance(action_space, gym.spaces.Discrete):\n",
        "            self.actor = nn.Linear(hidden_size, action_space.n)\n",
        "            self.is_discrete = True\n",
        "        elif isinstance(action_space, gym.spaces.Box):\n",
        "            self.actor_mean = nn.Linear(hidden_size, action_space.shape[0])\n",
        "            self.actor_log_std = nn.Parameter(torch.zeros(action_space.shape[0]))\n",
        "            self.is_discrete = False\n",
        "        else:\n",
        "            raise NotImplementedError(\"Unsupported action space type\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        value = self.critic(x)\n",
        "\n",
        "        if self.is_discrete:\n",
        "            logits = self.actor(x)\n",
        "            action_probs = F.softmax(logits, dim=-1)\n",
        "            return action_probs, value\n",
        "        else:\n",
        "            mean = self.actor_mean(x)\n",
        "            log_std = self.actor_log_std.expand_as(mean)\n",
        "            return (mean, log_std), value\n",
        "\n",
        "def create_shared_network(env, hidden_size=128):\n",
        "    obs_space = env.observation_space\n",
        "    if isinstance(obs_space, gym.spaces.Discrete):\n",
        "        input_dim = obs_space.n\n",
        "    elif isinstance(obs_space, gym.spaces.Box):\n",
        "        input_dim = int(torch.tensor(obs_space.shape).prod().item())\n",
        "    else:\n",
        "        raise NotImplementedError(\"Unsupported observation space type\")\n",
        "    return SharedNetwork(input_dim, env.action_space, hidden_size)\n",
        "\n",
        "# Use gymnasium environments only\n",
        "env_names = [\n",
        "    \"CliffWalking-v0\",         # Discrete obs/act\n",
        "    \"LunarLander-v3\",          # Box obs, discrete act\n",
        "    # \"PongNoFrameskip-v4\",    # Only if Atari and AutoROM set up\n",
        "    \"HalfCheetah-v5\",        # Requires MuJoCo, skip unless installed\n",
        "]\n",
        "\n",
        "for env_name in env_names:\n",
        "    try:\n",
        "        env = gym.make(env_name)\n",
        "        print(f\"\\n‚úÖ Creating network for {env_name}:\")\n",
        "        model = create_shared_network(env, hidden_size=128)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to create environment {env_name}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # Dummy input setup\n",
        "    if isinstance(env.observation_space, gym.spaces.Discrete):\n",
        "        dummy_input = F.one_hot(torch.tensor([0, 1, 2]), num_classes=env.observation_space.n).float()\n",
        "    elif isinstance(env.observation_space, gym.spaces.Box):\n",
        "        obs_shape = env.observation_space.shape\n",
        "        input_dim = int(torch.tensor(obs_shape).prod().item())\n",
        "        dummy_input = torch.randn(3, input_dim)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Unsupported observation space for {env_name}\")\n",
        "        continue\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(dummy_input)\n",
        "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
        "        action_probs, value = output\n",
        "        print(\"Actor Output (Action Probabilities):\", action_probs)\n",
        "        print(\"Critic Output (State Value):\", value)\n",
        "    elif isinstance(env.action_space, gym.spaces.Box):\n",
        "        (mean, log_std), value = output\n",
        "        print(\"Actor Output (Mean):\", mean)\n",
        "        print(\"Actor Output (Log Std):\", log_std)\n",
        "        print(\"Critic Output (State Value):\", value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33EwxaA8rNb0",
        "outputId": "a3f41d83-2d64-4dc2-ce20-8ed79137cf90"
      },
      "id": "33EwxaA8rNb0",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Creating network for CliffWalking-v0:\n",
            "Actor Output (Action Probabilities): tensor([[0.2622, 0.2393, 0.2595, 0.2390],\n",
            "        [0.2633, 0.2386, 0.2590, 0.2391],\n",
            "        [0.2636, 0.2410, 0.2608, 0.2347]], grad_fn=<SoftmaxBackward0>)\n",
            "Critic Output (State Value): tensor([[-0.0108],\n",
            "        [ 0.0163],\n",
            "        [ 0.0144]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "‚úÖ Creating network for LunarLander-v3:\n",
            "Actor Output (Action Probabilities): tensor([[0.2782, 0.2424, 0.2597, 0.2197],\n",
            "        [0.2309, 0.2520, 0.2414, 0.2757],\n",
            "        [0.2445, 0.2619, 0.2434, 0.2502]], grad_fn=<SoftmaxBackward0>)\n",
            "Critic Output (State Value): tensor([[0.4528],\n",
            "        [0.1845],\n",
            "        [0.2574]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "‚úÖ Creating network for HalfCheetah-v5:\n",
            "Actor Output (Mean): tensor([[ 0.0101,  0.0047, -0.0708, -0.0378, -0.0396,  0.0467],\n",
            "        [-0.1205, -0.0997,  0.0503, -0.0955, -0.0657,  0.2039],\n",
            "        [-0.0487,  0.0825,  0.0891, -0.2951, -0.0791,  0.0661]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Actor Output (Log Std): tensor([[0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.]], grad_fn=<ExpandBackward0>)\n",
            "Critic Output (State Value): tensor([[-0.1076],\n",
            "        [-0.1581],\n",
            "        [-0.0290]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "4ccd13f0b62b30ff"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "4ccd13f0b62b30ff"
    },
    {
      "metadata": {
        "id": "ee2dd81024ce246a"
      },
      "cell_type": "code",
      "source": [],
      "id": "ee2dd81024ce246a",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "b39c886fa536a639"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 3: Write Observation Normalization Function\n",
        "Create a function `normalize_observation(obs, env)` that:\n",
        "- Checks if the observation space is `Box` and has `low` and `high` attributes.\n",
        "- If so, normalize the input observation.\n",
        "- Otherwise, return the observation unchanged.\n",
        "\n",
        "```python\n",
        "# TODO: Define `normalize_observation(obs, env)`\n",
        "```\n",
        "\n",
        "Test this function with observations from:\n",
        "- `LunarLander-v3`\n",
        "- `PongNoFrameskip-v4`\n",
        "\n",
        "Note: Atari observations are image arrays. Normalize pixel values to [0, 1]. For LunarLander-v3, the different elements in the observation vector have different ranges. Normalize them to [0, 1] using the `low` and `high` attributes of the observation space.\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "b39c886fa536a639"
    },
    {
      "metadata": {
        "id": "fc7ee06112cf7d29"
      },
      "cell_type": "code",
      "source": [
        "import gymnasium as gymn  # For LunarLander-v3\n",
        "import gym               # For PongNoFrameskip-v4\n",
        "import numpy as np\n",
        "\n",
        "def normalize_observation(obs, env):\n",
        "    \"\"\"\n",
        "    Normalize an observation based on the observation space attributes.\n",
        "\n",
        "    - If `obs` is a dict, normalize each value.\n",
        "    - If `obs` is a list or tuple, attempt to stack if all elements have the same shape;\n",
        "      otherwise, process each element individually.\n",
        "    - For environments with a Box observation space:\n",
        "         - If the dtype is uint8 (e.g., Atari images), convert to float32 and scale by 1/255.\n",
        "         - Otherwise, normalize using the low/high bounds elementwise.\n",
        "    - The output is clipped to [0, 1].\n",
        "    \"\"\"\n",
        "    # Handle dict observations\n",
        "    if isinstance(obs, dict):\n",
        "        return {k: normalize_observation(v, env) for k, v in obs.items()}\n",
        "\n",
        "    # Handle sequence observations (list or tuple)\n",
        "    if isinstance(obs, (list, tuple)):\n",
        "        try:\n",
        "            # Try converting each element to a numpy array.\n",
        "            arrays = [np.array(o) for o in obs]\n",
        "            shapes = [a.shape for a in arrays]\n",
        "            # If all elements share the same shape, stack them.\n",
        "            if len(set(shapes)) == 1:\n",
        "                obs = np.stack(arrays, axis=0)\n",
        "            else:\n",
        "                # Otherwise, normalize each element individually and return the same type.\n",
        "                return type(obs)(normalize_observation(o, env) for o in obs)\n",
        "        except Exception:\n",
        "            # If any error occurs, normalize each element individually.\n",
        "            return type(obs)(normalize_observation(o, env) for o in obs)\n",
        "\n",
        "    # At this point, obs should be convertible to a numpy array.\n",
        "    try:\n",
        "        obs = np.asarray(obs, dtype=np.float32)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Unable to convert observation to numpy array: {e}\")\n",
        "\n",
        "    # If the observation space defines low and high, use them.\n",
        "    if hasattr(env.observation_space, 'low') and hasattr(env.observation_space, 'high'):\n",
        "        # For Atari environments (commonly uint8 images), scale by 255.\n",
        "        if hasattr(env.observation_space, 'dtype') and env.observation_space.dtype == np.uint8:\n",
        "            normalized = obs / 255.0\n",
        "        else:\n",
        "            low = env.observation_space.low.astype(np.float32)\n",
        "            high = env.observation_space.high.astype(np.float32)\n",
        "            normalized = (obs - low) / (high - low)\n",
        "        normalized = np.clip(normalized, 0, 1)\n",
        "        return normalized\n",
        "    else:\n",
        "        return obs\n"
      ],
      "id": "fc7ee06112cf7d29",
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "env_lunar = gymn.make(\"LunarLander-v3\")\n",
        "obs_lunar, _ = env_lunar.reset()  # Gymnasium returns (observation, info)\n",
        "norm_obs_lunar = normalize_observation(obs_lunar, env_lunar)\n",
        "print(\"LunarLander-v3 original observation:\", obs_lunar)\n",
        "print(\"LunarLander-v3 normalized observation:\", norm_obs_lunar)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkRTttmYtoiD",
        "outputId": "b38ed3a0-b2a5-4d00-81de-190582c19fb0"
      },
      "id": "BkRTttmYtoiD",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LunarLander-v3 original observation: [ 1.3543129e-03  1.4018842e+00  1.3715582e-01 -4.0159121e-01\n",
            " -1.5624668e-03 -3.1067912e-02  0.0000000e+00  0.0000000e+00]\n",
            "LunarLander-v3 normalized observation: [0.50027084 0.7803768  0.50685775 0.47992045 0.49987566 0.4984466\n",
            " 0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_pong = gym.make(\"PongNoFrameskip-v4\")\n",
        "obs_pong = env_pong.reset()  # Gym returns the observation directly.\n",
        "norm_obs_pong = normalize_observation(obs_pong, env_pong)\n",
        "\n",
        "# For display, check if the result is a sequence or an array.\n",
        "if isinstance(norm_obs_pong, (list, tuple)):\n",
        "    shapes = [np.asarray(frame).shape for frame in norm_obs_pong]\n",
        "    mins = [np.min(frame) for frame in norm_obs_pong]\n",
        "    maxs = [np.max(frame) for frame in norm_obs_pong]\n",
        "    print(\"PongNoFrameskip-v4 normalized observation shapes:\", shapes)\n",
        "    print(\"PongNoFrameskip-v4 normalized observation value ranges:\")\n",
        "    for i, (mi, ma) in enumerate(zip(mins, maxs)):\n",
        "        print(f\"  Frame {i}: {mi} to {ma}\")\n",
        "else:\n",
        "    print(\"PongNoFrameskip-v4 original observation shape:\", np.asarray(obs_pong).shape)\n",
        "    print(\"PongNoFrameskip-v4 normalized observation range:\",\n",
        "          norm_obs_pong.min(), \"to\", norm_obs_pong.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwPrVVdEvpnR",
        "outputId": "9fa94b63-5716-4d95-a12b-f904082fb9b3"
      },
      "id": "vwPrVVdEvpnR",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PongNoFrameskip-v4 normalized observation shapes: [(210, 160, 3), ()]\n",
            "PongNoFrameskip-v4 normalized observation value ranges:\n",
            "  Frame 0: 0.0 to 0.8941176533699036\n",
            "  Frame 1: {'lives': np.float32(0.0), 'episode_frame_number': np.float32(0.0), 'frame_number': np.float32(0.0)} to {'lives': np.float32(0.0), 'episode_frame_number': np.float32(0.0), 'frame_number': np.float32(0.0)}\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "501ed2a6e7ca7a7b"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "501ed2a6e7ca7a7b"
    },
    {
      "metadata": {
        "id": "78211b617a843f62"
      },
      "cell_type": "code",
      "source": [],
      "id": "78211b617a843f62",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "6b5fb5353307f514"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 4: Gradient Clipping\n",
        "\n",
        "To prevent exploding gradients, it's common practice to clip gradients before optimizer updates.\n",
        "\n",
        "### Task 4: Clip Gradients for Actor-Critic Networks\n",
        "Use dummy tensors and apply gradient clipping with the following PyTorch method:\n",
        "```python\n",
        "# During training, after loss.backward():\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "```\n",
        "\n",
        "Reuse the loss computation from Task 1a or 1b. After computing the gradients, apply gradient clipping.\n",
        "Print the gradient norm before and after clipping to verify it‚Äôs applied.\n",
        "\n",
        "üîó PyTorch Docs: https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "6b5fb5353307f514"
    },
    {
      "metadata": {
        "id": "7327507fb6e803ad"
      },
      "cell_type": "code",
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "\n",
        "class DummyActorCritic(nn.Module):\n",
        "    def __init__(self, input_size, num_actions):\n",
        "        super(DummyActorCritic, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, 128)\n",
        "        self.actor = nn.Linear(128, num_actions)\n",
        "        self.critic = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc(x))\n",
        "        # Actor outputs action probabilities via softmax.\n",
        "        action_probs = F.softmax(self.actor(x), dim=-1)\n",
        "        # Critic outputs a single value.\n",
        "        value = self.critic(x)\n",
        "        return action_probs, value\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "7327507fb6e803ad",
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_training_with_grad_clip(model, state):\n",
        "    \"\"\"\n",
        "    Simulate training using dummy tensors with a single optimizer for both Actor and Critic.\n",
        "    After computing the gradients, gradient clipping is applied.\n",
        "    The gradient norm is printed before and after clipping.\n",
        "    \"\"\"\n",
        "    # Forward pass through the model\n",
        "    action_probs, value = model(state)\n",
        "\n",
        "    # --- Dummy tensor simulation ---\n",
        "    # Simulate dummy log probabilities from the actor; assume a batch of actions.\n",
        "    dummy_log_probs = torch.log(action_probs + 1e-8)\n",
        "\n",
        "    # Simulated returns (e.g., discounted rewards)\n",
        "    dummy_returns = torch.randn_like(value)\n",
        "\n",
        "    # Simulated dummy entropies (for the actor)\n",
        "    dummy_entropies = -torch.sum(action_probs * dummy_log_probs, dim=1, keepdim=True)\n",
        "\n",
        "    # Compute advantage: (return - value)\n",
        "    advantage = dummy_returns - value.detach()\n",
        "\n",
        "    # Actor loss: negative log probability * advantage minus entropy bonus (with a factor of 0.01)\n",
        "    actor_loss = -(dummy_log_probs * advantage).mean() - 0.01 * dummy_entropies.mean()\n",
        "\n",
        "    # Critic loss: mean squared error between value and return\n",
        "    critic_loss = F.mse_loss(value, dummy_returns)\n",
        "\n",
        "    # Total loss as weighted sum (here we assume equal weighting)\n",
        "    total_loss = actor_loss + critic_loss\n",
        "\n",
        "    # Create a single optimizer for both actor and critic\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    total_loss.backward()\n",
        "\n",
        "    # Compute gradient norm before clipping\n",
        "    pre_clip_norm = 0.0\n",
        "    for p in model.parameters():\n",
        "        if p.grad is not None:\n",
        "            pre_clip_norm += p.grad.data.norm(2).item() ** 2\n",
        "    pre_clip_norm = pre_clip_norm ** 0.5\n",
        "\n",
        "    # Apply gradient clipping: returns total norm before clipping (for reference)\n",
        "    clip_norm = utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "\n",
        "    # Compute gradient norm after clipping\n",
        "    post_clip_norm = 0.0\n",
        "    for p in model.parameters():\n",
        "        if p.grad is not None:\n",
        "            post_clip_norm += p.grad.data.norm(2).item() ** 2\n",
        "    post_clip_norm = post_clip_norm ** 0.5\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    print(\"Single Optimizer Training Step:\")\n",
        "    print(\"Actor Loss:\", actor_loss.item())\n",
        "    print(\"Critic Loss:\", critic_loss.item())\n",
        "    print(\"Total Loss:\", total_loss.item())\n",
        "    print(\"Gradient norm before clipping:\", pre_clip_norm)\n",
        "    print(\"Gradient norm reported by clip_grad_norm_ (before clipping):\", clip_norm)\n",
        "    print(\"Gradient norm after clipping:\", post_clip_norm)"
      ],
      "metadata": {
        "id": "1ZtjO1_1xSx9"
      },
      "id": "1ZtjO1_1xSx9",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 8     # Example input dimension (e.g., LunarLander-v3 state size)\n",
        "num_actions = 4    # Example number of actions\n",
        "model = DummyActorCritic(input_size, num_actions)\n",
        "\n",
        "# Create a dummy state tensor (batch size 1)\n",
        "state = torch.randn(1, input_size)\n",
        "\n",
        "simulate_training_with_grad_clip(model, state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtSAd_w-xVb5",
        "outputId": "6f0dd12e-0088-48ff-96fd-932a92855368"
      },
      "id": "RtSAd_w-xVb5",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Optimizer Training Step:\n",
            "Actor Loss: -2.9831323623657227\n",
            "Critic Loss: 4.527118682861328\n",
            "Total Loss: 1.5439863204956055\n",
            "Gradient norm before clipping: 23.084273351243823\n",
            "Gradient norm reported by clip_grad_norm_ (before clipping): tensor(23.0843)\n",
            "Gradient norm after clipping: 0.4999999788338938\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "9952750fa74cd487"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "9952750fa74cd487"
    },
    {
      "metadata": {
        "id": "557a9303f5a1c863"
      },
      "cell_type": "code",
      "source": [],
      "id": "557a9303f5a1c863",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "f4cff31e6c6e7e4a"
      },
      "cell_type": "markdown",
      "source": [
        "If you are working in a team, provide a contribution summary.\n",
        "| Team Member | Step# | Contribution (%) |\n",
        "|---|---|---|\n",
        "|   | Task 1 |   |\n",
        "|   | Task 2 |   |\n",
        "|   | Task 3 |   |\n",
        "|   | Task 4 |   |\n",
        "|   | **Total** |   |\n"
      ],
      "id": "f4cff31e6c6e7e4a"
    },
    {
      "metadata": {
        "id": "4be0a6e29f281e23"
      },
      "cell_type": "code",
      "source": [],
      "id": "4be0a6e29f281e23",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}