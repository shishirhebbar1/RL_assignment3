{
  "cells": [
    {
      "metadata": {
        "id": "cf4d8c9b006ada45"
      },
      "cell_type": "markdown",
      "source": [
        "## <center>CSE 546: Reinforcement Learning</center>\n",
        "### <center>Prof. Alina Vereshchaka</center>\n",
        "#### <center>Spring 2025</center>\n",
        "\n",
        "Welcome to the Assignment 3, Part 1: Introduction to Actor-Critic Methods! It includes the implementation of simple actor and critic networks and best practices used in modern Actor-Critic algorithms."
      ],
      "id": "cf4d8c9b006ada45"
    },
    {
      "metadata": {
        "id": "9d7a6d891e2fb312"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 0: Setup and Imports"
      ],
      "id": "9d7a6d891e2fb312"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53473293aa9daf8e",
        "outputId": "65b75269-de32-4693-d2d0-0b11022950ca"
      },
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\"\n",
        "!pip install gymnasium[atari,accept-rom-license]\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils as utils\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "id": "53473293aa9daf8e",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.9 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.0\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379445 sha256=ce3a792dc014019c6288bc0f677d0eb54211433e85174c090ec8d24fb7561b55\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "\u001b[33mWARNING: gymnasium 1.1.1 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license,atari]) (0.10.2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ecd0c32e4f0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "execution_count": 6
    },
    {
      "metadata": {
        "id": "2a3d9c34ff222994"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 1: Actor-Critic Network Architectures and Loss Computation\n",
        "\n",
        "In this section, you will explore two common architectural designs for Actor-Critic methods and implement their corresponding loss functions using dummy tensors. These architectures are:\n",
        "- A. Completely separate actor and critic networks\n",
        "- B. A shared network with two output heads\n",
        "\n",
        "Both designs are widely used in practice. Shared networks are often more efficient and generalize better, while separate networks offer more control and flexibility.\n",
        "\n",
        "---\n"
      ],
      "id": "2a3d9c34ff222994"
    },
    {
      "metadata": {
        "id": "971fa7887dd4f858"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 1a â€“ Separate Actor and Critic Networks with Loss Function\n",
        "\n",
        "Define a class `SeparateActorCritic`. Your goal is to:\n",
        "- Create two completely independent neural networks: one for the actor and one for the critic.\n",
        "- The actor should output a probability distribution over discrete actions (use `nn.Softmax`).\n",
        "- The critic should output a single scalar value.\n",
        "\n",
        " Use `nn.ReLU()` as your activation function. Include at least one hidden layer of reasonable width (e.g. 64 or 128 units).\n",
        "\n",
        "```python\n",
        "# TODO: Define SeparateActorCritic class\n",
        "```\n",
        "\n",
        " Next, simulate training using dummy tensors:\n",
        "1. Generate dummy tensors for log-probabilities, returns, estimated values, and entropies.\n",
        "2. Compute the actor loss using the advantage (return - value).\n",
        "3. Compute the critic loss as mean squared error between values and returns.\n",
        "4. Use a single optimizer for both the Actor and the Critic. In this case, combine the actor and critic losses into a total loss and perform backpropagation.\n",
        "5. Use a separate optimizers for both the Actor and the Critic. In this case, keep the actor and critic losses separate and perform backpropagation.\n",
        "\n",
        "```python\n",
        "# TODO: Simulate loss computation and backpropagation\n",
        "```\n",
        "\n",
        "ðŸ”— Helpful references:\n",
        "- PyTorch Softmax: https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
        "- PyTorch MSE Loss: https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html\n",
        "\n",
        "---"
      ],
      "id": "971fa7887dd4f858"
    },
    {
      "metadata": {
        "id": "dd6b81ed1791e4e6"
      },
      "cell_type": "code",
      "source": [
        "# TODO: Define a class SeparateActorCritic with separate networks for actor and critic\n",
        "\n",
        "# BEGIN_YOUR_CODE\n",
        "class SeparateActorCritic(nn.Module):\n",
        "    def __init__(self, state_dimension, action_dimension, hidden_size=64):\n",
        "        super(SeparateActorCritic, self).__init__()\n",
        "        self.actor_fc1 = nn.Linear(state_dimension, hidden_size)\n",
        "        self.actor_fc2 = nn.Linear(hidden_size, action_dimension)\n",
        "        self.actor_softmax = nn.Softmax(dim=-1)\n",
        "        self.critic_fc1 = nn.Linear(state_dimension, hidden_size)\n",
        "        self.critic_fc2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        actor_hidden = F.relu(self.actor_fc1(state))\n",
        "        action_logits = self.actor_fc2(actor_hidden)\n",
        "        action_probabilities = self.actor_softmax(action_logits)\n",
        "        critic_hidden = F.relu(self.critic_fc1(state))\n",
        "        value = self.critic_fc2(critic_hidden)\n",
        "        return action_probabilities, value\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "dd6b81ed1791e4e6",
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_training(model, state):\n",
        "    action_probabilities, value = model(state)\n",
        "    dummy_log_probabilities = torch.log(action_probabilities + 1e-8)\n",
        "    dummy_return_values = torch.randn_like(value)\n",
        "    dummy_entropies = -torch.sum(action_probabilities * dummy_log_probabilities, dim=1, keepdim=True)\n",
        "    advantage = dummy_return_values - value.detach()\n",
        "    actor_loss = -(dummy_log_probabilities * advantage).mean() - 0.01 * dummy_entropies.mean()\n",
        "    critic_loss = F.mse_loss(value, dummy_return_values)\n",
        "    total_loss = actor_loss + critic_loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"Single Optimizer:\")\n",
        "    print(\"Actor Loss:\", actor_loss.item())\n",
        "    print(\"Critic Loss:\", critic_loss.item())\n",
        "    print(\"Total Loss:\", total_loss.item())\n",
        "\n",
        "\n",
        "def simulate_training_separate_optimizers(model, state):\n",
        "    action_probabilities, value = model(state)\n",
        "    dummy_log_probabilities = torch.log(action_probabilities + 1e-8)\n",
        "    dummy_return_values = torch.randn_like(value)\n",
        "    dummy_entropies = -torch.sum(action_probabilities * dummy_log_probabilities, dim=1, keepdim=True)\n",
        "    advantage = dummy_return_values - value.detach()\n",
        "    actor_loss = -(dummy_log_probabilities * advantage).mean() - 0.01 * dummy_entropies.mean()\n",
        "    critic_loss = F.mse_loss(value, dummy_return_values)\n",
        "    actor_params = list(model.actor_fc1.parameters()) + list(model.actor_fc2.parameters())\n",
        "    critic_params = list(model.critic_fc1.parameters()) + list(model.critic_fc2.parameters())\n",
        "    optimizer_actor = optim.Adam(actor_params, lr=1e-3)\n",
        "    optimizer_critic = optim.Adam(critic_params, lr=1e-3)\n",
        "    optimizer_actor.zero_grad()\n",
        "    actor_loss.backward(retain_graph=True)\n",
        "    optimizer_actor.step()\n",
        "    optimizer_critic.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    optimizer_critic.step()\n",
        "    print(\"\\nSeparate Optimizers:\")\n",
        "    print(\"Actor Loss:\", actor_loss.item())\n",
        "    print(\"Critic Loss:\", critic_loss.item())"
      ],
      "metadata": {
        "id": "-zYQo-UgYdyO"
      },
      "id": "-zYQo-UgYdyO",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dimension = 10\n",
        "action_dimension = 4\n",
        "batch_size = 5\n",
        "dummy_state = torch.randn(batch_size, state_dimension)\n",
        "model = SeparateActorCritic(state_dimension, action_dimension, hidden_size=64)\n",
        "simulate_training(model, dummy_state)\n",
        "simulate_training_separate_optimizers(model, dummy_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwIofiUFYnWG",
        "outputId": "d5110cb4-11ef-4e0c-cbb9-73c82025bd25"
      },
      "id": "gwIofiUFYnWG",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Optimizer:\n",
            "Actor Loss: 0.29983144998550415\n",
            "Critic Loss: 2.5514073371887207\n",
            "Total Loss: 2.85123872756958\n",
            "\n",
            "Separate Optimizers:\n",
            "Actor Loss: -0.46809422969818115\n",
            "Critic Loss: 1.2666232585906982\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "eb8e90c88108cd2e"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "eb8e90c88108cd2e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In actorâ€“critic reinforcement learning architectures, two distinct components are trained simultaneously: the actor, which learns a policy for selecting actions, and the critic, which estimates the value of states or stateâ€“action pairs to guide the actor's learning. When both components are optimized using a single optimizer, the setup remains simple and compact. This approach is especially practical when the actor and critic share a common feature extractor or network trunk. In such cases, a single backward pass can efficiently update all parameters using a combined loss functionâ€”typically the sum of the actorâ€™s and criticâ€™s losses. This method is often preferred in simpler environments or during early-stage experimentation, as it reduces implementation complexity and the number of hyperparameters to tune.\n",
        "\n",
        "However, using separate optimizers for the actor and critic introduces greater flexibility. This setup allows each component to be trained with its own learning rate, optimizer type, or update frequency, which can be crucial in more complex environments. For instance, the critic may require faster learning to provide timely and accurate value estimates, while the actor may benefit from slower, more stable updates to its policy. With separate optimizers, it is easier to manage such divergent learning dynamics without one loss overpowering the other. This method is particularly valuable when the actor and critic do not share network components, or when their losses scale differently over time.\n",
        "\n",
        "In summary, the single-optimizer approach is generally preferred for its simplicity and efficiency in shared-network settings, while the separate-optimizers setup is advantageous in scenarios demanding more control over the individual learning behaviors of the actor and critic. The choice between the two depends on the complexity of the task, the network architecture, and the desired level of control over training dynamics."
      ],
      "metadata": {
        "id": "v-kXvLklV6QL"
      },
      "id": "v-kXvLklV6QL"
    },
    {
      "metadata": {
        "id": "64081a606b93029d"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 1b â€“ Shared Network with Actor and Critic Heads + Loss Function\n",
        "\n",
        "Now define a class `SharedActorCritic`:\n",
        "- Build a shared base network (e.g., linear layer + ReLU)\n",
        "- Create two heads: one for actor (output action probabilities) and one for critic (output state value)\n",
        "\n",
        "```python\n",
        "# TODO: Define SharedActorCritic class\n",
        "```\n",
        "\n",
        "Then:\n",
        "1. Pass a dummy input tensor through the model to obtain action probabilities and value.\n",
        "2. Simulate dummy rewards and compute advantage.\n",
        "3. Compute the actor and critic losses, combine them, and backpropagate.\n",
        "\n",
        "```python\n",
        "# TODO: Simulate shared network loss computation and backpropagation\n",
        "```\n",
        "\n",
        " Use `nn.Softmax` for actor output and `nn.Linear` for scalar critic output.\n",
        "\n",
        "ðŸ”— More reading:\n",
        "- Policy Gradient Methods: https://spinningup.openai.com/en/latest/algorithms/vpg.html\n",
        "- Actor-Critic Overview: https://www.tensorflow.org/agents/tutorials/6_reinforce_tutorial\n",
        "- PyTorch Categorical Distribution: https://pytorch.org/docs/stable/distributions.html#categorical\n",
        "\n",
        "---"
      ],
      "id": "64081a606b93029d"
    },
    {
      "metadata": {
        "id": "a48f882fff11aecc"
      },
      "cell_type": "code",
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "class SharedActorCritic(nn.Module):\n",
        "    def __init__(self, state_dimension, action_dimension, hidden_size=64):\n",
        "        super(SharedActorCritic, self).__init__()\n",
        "        self.shared_fc = nn.Linear(state_dimension, hidden_size)\n",
        "        self.actor_fc = nn.Linear(hidden_size, action_dimension)\n",
        "        self.actor_softmax = nn.Softmax(dim=-1)\n",
        "        self.critic_fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        shared_hidden = F.relu(self.shared_fc(state))\n",
        "        action_logits = self.actor_fc(shared_hidden)\n",
        "        action_probabilities = self.actor_softmax(action_logits)\n",
        "        value = self.critic_fc(shared_hidden)\n",
        "        return action_probabilities, value\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "a48f882fff11aecc",
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_training_on_shared_network(model, state):\n",
        "    action_probabilities, value = model(state)\n",
        "    dummy_log_probabilities = torch.log(action_probabilities + 1e-8)\n",
        "    dummy_return_values = torch.randn_like(value)\n",
        "    advantage = dummy_return_values - value.detach()\n",
        "    dummy_entropies = -torch.sum(action_probabilities * dummy_log_probabilities, dim=1, keepdim=True)\n",
        "    actor_loss = -(dummy_log_probabilities * advantage).mean() - 0.01 * dummy_entropies.mean()\n",
        "    critic_loss = F.mse_loss(value, dummy_return_values)\n",
        "    total_loss = actor_loss + critic_loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"Shared Network Training:\")\n",
        "    print(\"Actor Loss:\", actor_loss.item())\n",
        "    print(\"Critic Loss:\", critic_loss.item())\n",
        "    print(\"Total Loss:\", total_loss.item())"
      ],
      "metadata": {
        "id": "BWcfMCwzZeMG"
      },
      "id": "BWcfMCwzZeMG",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dimension = 10\n",
        "action_dimension = 4\n",
        "batch_size = 5\n",
        "dummy_state = torch.randn(batch_size, state_dimension)\n",
        "model = SharedActorCritic(state_dimension, action_dimension, hidden_size=64)\n",
        "simulate_training_on_shared_network(model, dummy_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lNYmqzHZhtR",
        "outputId": "f3964abe-a8f1-4569-b23d-a468ad3b8a93"
      },
      "id": "9lNYmqzHZhtR",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shared Network Training:\n",
            "Actor Loss: -0.4370136857032776\n",
            "Critic Loss: 0.8117109537124634\n",
            "Total Loss: 0.3746972680091858\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "a974e302d1fdb028"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "a974e302d1fdb028"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shared actorâ€“critic architecture employs a single â€œbackboneâ€ network to extract features from each state, which are then consumed by both the policy (actor) and the value function (critic). By sharing this feature extractor, the criticâ€™s gradient signals naturally inform the actor about which aspects of the state space are most relevant, often yielding faster and more sampleâ€‘efficient learning. This parameterâ€andâ€compute efficiency proves particularly advantageous in lowâ€‘data regimes or on resourceâ€‘constrained devices. However, when the objectives of policy improvement and value estimation diverge substantially, the shared backbone can introduce destructive interferenceâ€”at which point fully separate networks may be more appropriate.\n",
        "\n",
        "From the shared representation, the actor head produces logits for each discrete action and applies a softmax to obtain a valid probability distribution. Operating in logit space simplifies the incorporation of temperature scaling for exploration control and the computation of entropy regularization to discourage premature convergence to deterministic policies. In domains that require continuous control, this softmax head would be replaced by a Gaussian parameterization (learning both means and variances), while very large discrete action spaces might leverage sampling approximations such as Gumbelâ€‘Softmax to contain computational costs.\n",
        "\n",
        "Simultaneously, the critic head maps the shared features to a single scalar value. Training proceeds by minimizing the meanâ€‘squared error between this estimate and the empirically observed return. To guide the actor, an advantage is computed by subtracting the criticâ€™s detached estimate from the actual return, supplying a lowerâ€‘variance policy gradient without bias. In particularly noisy or outlierâ€‘prone environments, practitioners may swap the MSE loss for a Huber loss to improve robustness, and employ multiâ€‘step returns or Generalized Advantage Estimation (GAE) to refine the biasâ€“variance characteristics of the advantage signal.\n",
        "\n",
        "An entropy bonusâ€”scaled in this implementation by a factor of 0.01â€”is added to the actorâ€™s loss to maintain sufficient exploration throughout training. Early in learning, a higher entropy coefficient can encourage broader policy coverage, whereas annealing it toward zero supports convergence to the highestâ€‘reward behaviors. This straightforward mechanism helps navigate the explorationâ€“exploitation tradeâ€‘off, especially in sparseâ€‘reward or deceptive environments.\n",
        "\n",
        "Finally, actor and critic losses are summed into a single optimization objective and updated jointly using Adam at a modest learning rate of 1Â Ã—Â 10â»Â³. Joint backpropagation through the shared backbone streamlines implementation and leverages coupled gradient information. For largeâ€‘scale or highly sensitive tasks, however, one might adopt separate optimizers or learningâ€‘rate schedules for each head, introduce gradient clipping to prevent instability, or revert to RMSPropâ€”as seen in A2C/A3C implementationsâ€”if it yields superior empirical performance.\n",
        "\n",
        "In practice, the choice between full sharing, partial sharing, or complete decoupling of actor and critic networks depends on factors such as compute budget, data availability, and the degree to which policy and value estimation benefit from common features. A fully shared network with MSE critic loss and a modest entropy bonus serves as a robust default; more advanced scenarios may demand tailored advantage estimators, robust critic losses, or even distributional critics to capture richer value information."
      ],
      "metadata": {
        "id": "51iIQXbSWnES"
      },
      "id": "51iIQXbSWnES"
    },
    {
      "metadata": {
        "id": "eb645eb009b85b1c"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 2: Auto-Adaptive Network Setup for Environments\n",
        "\n",
        "You will now create a function that builds a shared actor-critic network that adapts to any Gymnasium environment. This function should inspect the environment and build input/output layers accordingly."
      ],
      "id": "eb645eb009b85b1c"
    },
    {
      "metadata": {
        "id": "4223b6ddf43abee5"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 2: Auto-generate Input and Output Layers\n",
        "Write a function `create_shared_network(env)` that constructs a neural network using the following rules:\n",
        "- The input layer should match the environment's observation space.\n",
        "- The output layer for the **actor** should depend on the action space:\n",
        "  - For discrete actions: output probabilities using `nn.Softmax`.\n",
        "  - For continuous actions: output mean and log std for a Gaussian distribution.\n",
        "- The **critic** always outputs a single scalar value.\n",
        "\n",
        "```python\n",
        "# TODO: Define function `create_shared_network(env)`\n",
        "```\n",
        "\n",
        "#### Environments to Support:\n",
        "Test your function with the following environments:\n",
        "1. `CliffWalking-v0` (Use one-hot encoding for discrete integer observations.)\n",
        "2. `LunarLander-v3` (Standard Box space for observations and discrete actions.)\n",
        "3. `PongNoFrameskip-v4` (Use gym wrappers for Atari image preprocessing.)\n",
        "4. `HalfCheetah-v5` (Continuous observation and continuous action.)\n",
        "\n",
        "```python\n",
        "# TODO: Loop through environments and test `create_shared_network`\n",
        "```\n",
        "\n",
        "Hint: Use `gym.spaces` utilities to determine observation/action types dynamically.\n",
        "\n",
        "ðŸ”— Observation/Action Space Docs:\n",
        "- https://gymnasium.farama.org/api/spaces/\n",
        "\n",
        "---"
      ],
      "id": "4223b6ddf43abee5"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"gym[atari,accept-rom-license]\" ale-py autorom\n",
        "!AutoROM --accept-license"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "21lWySd7Gt8N",
        "outputId": "61e77d1e-5109-4e35-a0ce-f2f2be9eba4b"
      },
      "id": "21lWySd7Gt8N",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "Requirement already satisfied: autorom in /usr/local/lib/python3.11/dist-packages (0.6.1)\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.11/dist-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym[accept-rom-license,atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym[accept-rom-license,atari]) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting ale-py\n",
            "  Using cached ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting autorom\n",
            "  Using cached AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from ale-py) (6.5.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom) (8.1.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from autorom) (4.67.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom) (2025.1.31)\n",
            "Using cached ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "Using cached AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: ale-py, autorom\n",
            "  Attempting uninstall: ale-py\n",
            "    Found existing installation: ale-py 0.10.2\n",
            "    Uninstalling ale-py-0.10.2:\n",
            "      Successfully uninstalled ale-py-0.10.2\n",
            "  Attempting uninstall: autorom\n",
            "    Found existing installation: AutoROM 0.6.1\n",
            "    Uninstalling AutoROM-0.6.1:\n",
            "      Successfully uninstalled AutoROM-0.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires ale-py>=0.10.1, but you have ale-py 0.8.1 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ale-py-0.8.1 autorom-0.4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ale_py"
                ]
              },
              "id": "39d729b42f5e41818f8f8ee79922ced2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.11/dist-packages/AutoROM/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/adventure.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/air_raid.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/alien.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/amidar.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/assault.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/asterix.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/asteroids.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/atlantis.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/atlantis2.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/backgammon.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/bank_heist.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/basic_math.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/battle_zone.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/beam_rider.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/berzerk.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/blackjack.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/bowling.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/boxing.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/breakout.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/carnival.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/casino.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/centipede.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/chopper_command.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/combat.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/crazy_climber.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/crossbow.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/darkchambers.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/defender.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/demon_attack.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/donkey_kong.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/double_dunk.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/earthworld.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/elevator_action.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/enduro.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/entombed.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/et.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/fishing_derby.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/flag_capture.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/freeway.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/frogger.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/frostbite.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/galaxian.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/gopher.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/gravitar.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/hangman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/haunted_house.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/hero.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/human_cannonball.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/ice_hockey.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/jamesbond.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/journey_escape.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/joust.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kaboom.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kangaroo.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/keystone_kapers.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/king_kong.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/klax.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/koolaid.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/krull.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/kung_fu_master.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/laser_gates.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/lost_luggage.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/mario_bros.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/maze_craze.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/miniature_golf.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/montezuma_revenge.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/mr_do.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/ms_pacman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/name_this_game.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/othello.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pacman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/phoenix.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pitfall.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pitfall2.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pong.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/pooyan.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/private_eye.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/qbert.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/riverraid.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/road_runner.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/robotank.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/seaquest.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/sir_lancelot.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/skiing.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/solaris.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/space_invaders.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/space_war.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/star_gunner.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/superman.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/surround.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tennis.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tetris.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tic_tac_toe_3d.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/time_pilot.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/trondead.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/turmoil.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/tutankham.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/up_n_down.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/venture.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_checkers.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_chess.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_cube.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/video_pinball.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/warlords.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/wizard_of_wor.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/word_zapper.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/yars_revenge.bin\n",
            "Installed /usr/local/lib/python3.11/dist-packages/AutoROM/roms/zaxxon.bin\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.envs.registration import registry\n",
        "pong_envs = [env_id for env_id in registry.keys() if \"Pong\" in env_id]\n",
        "print(\"Pong env IDs:\", pong_envs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ_eFkeuG7WB",
        "outputId": "6fc03289-a078-48a2-e4bf-4816290b37f7"
      },
      "id": "FQ_eFkeuG7WB",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pong env IDs: ['ALE/Pong-v5', 'ALE/Pong-ram-v5', 'Pong-v0', 'PongDeterministic-v0', 'PongNoFrameskip-v0', 'Pong-v4', 'PongDeterministic-v4', 'PongNoFrameskip-v4', 'Pong-ram-v0', 'Pong-ramDeterministic-v0', 'Pong-ramNoFrameskip-v0', 'Pong-ram-v4', 'Pong-ramDeterministic-v4', 'Pong-ramNoFrameskip-v4']\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "d6d249ff9277403a"
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Actor_Critic_Network(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_size=128, is_continuous=False):\n",
        "        super(Actor_Critic_Network, self).__init__()\n",
        "        self.is_continuous = is_continuous\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
        "        self.actor = nn.Linear(hidden_size, output_dim if not is_continuous else hidden_size)\n",
        "        self.critic = nn.Linear(hidden_size, 1)\n",
        "        if is_continuous:\n",
        "            self.log_std = nn.Parameter(torch.zeros(output_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        value = self.critic(x)\n",
        "        if self.is_continuous:\n",
        "            mean = self.actor(x)\n",
        "            return (mean, self.log_std.exp()), value\n",
        "        else:\n",
        "            logits = self.actor(x)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            return probs, value"
      ],
      "id": "d6d249ff9277403a",
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "def create_shared_network(env, hidden_size=128):\n",
        "    observation_space = env.observation_space\n",
        "    action_space = env.action_space\n",
        "    if isinstance(observation_space, gym.spaces.Discrete):\n",
        "        input_dim = observation_space.n\n",
        "    else:\n",
        "        input_dim = int(torch.prod(torch.tensor(observation_space.shape)))\n",
        "    is_continuous = isinstance(action_space, gym.spaces.Box)\n",
        "    output_dim = action_space.shape[0] if is_continuous else action_space.n\n",
        "    return Actor_Critic_Network(input_dim, output_dim, hidden_size, is_continuous)"
      ],
      "metadata": {
        "id": "L6zW1URUa_U4"
      },
      "id": "L6zW1URUa_U4",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "environment_names = [\n",
        "    \"PongNoFrameskip-v4\",\n",
        "]\n",
        "\n",
        "for environment_name in environment_names:\n",
        "    try:\n",
        "        env = gym.make(environment_name)\n",
        "        print(f\"\\nCreating network for {environment_name}:\")\n",
        "        model = create_shared_network(env, hidden_size=128)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create environment {environment_name}: {e}\")\n",
        "        continue\n",
        "    if isinstance(env.observation_space, gym.spaces.Discrete):\n",
        "        dummy_input = torch.nn.functional.one_hot(torch.tensor([0, 1, 2]), num_classes=env.observation_space.n).float()\n",
        "    elif isinstance(env.observation_space, gym.spaces.Box):\n",
        "        shape = env.observation_space.shape\n",
        "        input_dim = int(torch.prod(torch.tensor(shape)))\n",
        "        dummy_input = torch.randn(3, input_dim)\n",
        "    else:\n",
        "        print(\"Unsupported observation space.\")\n",
        "        continue\n",
        "    output = model(dummy_input)\n",
        "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
        "        action_probabilities, value = output\n",
        "        print(\"Actor Output (Action Probabilities):\", action_probabilities)\n",
        "        print(\"Critic Output (State Value):\", value)\n",
        "    elif isinstance(env.action_space, gym.spaces.Box):\n",
        "        (mean, std), value = output\n",
        "        print(\"Actor Output (Mean):\", mean)\n",
        "        print(\"Actor Output (Std):\", std)\n",
        "        print(\"Critic Output (State Value):\", value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iSzCbarbDq3",
        "outputId": "ef8bcc27-69d2-4aed-ace0-0caf3f1fc2df"
      },
      "id": "9iSzCbarbDq3",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating network for PongNoFrameskip-v4:\n",
            "Actor Output (Action Probabilities): tensor([[0.1646, 0.1336, 0.0849, 0.1604, 0.2226, 0.2339],\n",
            "        [0.1832, 0.1227, 0.1208, 0.1979, 0.1802, 0.1952],\n",
            "        [0.1405, 0.1414, 0.0984, 0.2110, 0.2078, 0.2008]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Critic Output (State Value): tensor([[-0.4015],\n",
            "        [-0.0230],\n",
            "        [-0.1364]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"gymnasium[mujoco]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EcvxyyJrVae",
        "outputId": "ed004962-becd-4b2b-ac3f-f33ea24445ff"
      },
      "id": "0EcvxyyJrVae",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.2)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[mujoco])\n",
            "  Downloading glfw-2.9.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.21.0)\n",
            "Downloading mujoco-3.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.9.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.9.0 mujoco-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SharedNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, action_space, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.critic = nn.Linear(hidden_size, 1)\n",
        "        if isinstance(action_space, gym.spaces.Discrete):\n",
        "            self.actor = nn.Linear(hidden_size, action_space.n)\n",
        "            self.is_discrete = True\n",
        "        elif isinstance(action_space, gym.spaces.Box):\n",
        "            self.actor_mean = nn.Linear(hidden_size, action_space.shape[0])\n",
        "            self.actor_log_std = nn.Parameter(torch.zeros(action_space.shape[0]))\n",
        "            self.is_discrete = False\n",
        "        else:\n",
        "            raise NotImplementedError(\"Unsupported action space type\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        value = self.critic(x)\n",
        "        if self.is_discrete:\n",
        "            logits = self.actor(x)\n",
        "            action_probabilities = F.softmax(logits, dim=-1)\n",
        "            return action_probabilities, value\n",
        "        else:\n",
        "            mean = self.actor_mean(x)\n",
        "            log_std = self.actor_log_std.expand_as(mean)\n",
        "            return (mean, log_std), value\n",
        "\n",
        "def create_shared_network(env, hidden_size=128):\n",
        "    observation_space = env.observation_space\n",
        "    if isinstance(observation_space, gym.spaces.Discrete):\n",
        "        input_dim = observation_space.n\n",
        "    elif isinstance(observation_space, gym.spaces.Box):\n",
        "        input_dim = int(torch.tensor(observation_space.shape).prod().item())\n",
        "    else:\n",
        "        raise NotImplementedError(\"Unsupported observation space type\")\n",
        "    return SharedNetwork(input_dim, env.action_space, hidden_size)\n",
        "\n",
        "environment_names = [\n",
        "    \"CliffWalking-v0\",\n",
        "    \"LunarLander-v3\",\n",
        "    \"PongNoFrameskip-v4\",\n",
        "    \"HalfCheetah-v5\",\n",
        "]\n",
        "\n",
        "for environment_name in environment_names:\n",
        "    try:\n",
        "        env = gym.make(environment_name)\n",
        "        print(f\"\\n Creating network for {environment_name}:\")\n",
        "        model = create_shared_network(env, hidden_size=128)\n",
        "    except Exception as e:\n",
        "        print(f\" Failed to create environment {environment_name}: {e}\")\n",
        "        continue\n",
        "    if isinstance(env.observation_space, gym.spaces.Discrete):\n",
        "        dummy_input = F.one_hot(torch.tensor([0, 1, 2]), num_classes=env.observation_space.n).float()\n",
        "    elif isinstance(env.observation_space, gym.spaces.Box):\n",
        "        obs_shape = env.observation_space.shape\n",
        "        input_dim = int(torch.tensor(obs_shape).prod().item())\n",
        "        dummy_input = torch.randn(3, input_dim)\n",
        "    else:\n",
        "        print(f\"Unsupported observation space for {environment_name}\")\n",
        "        continue\n",
        "    output = model(dummy_input)\n",
        "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
        "        action_probabilities, value = output\n",
        "        print(\"Actor Output (Action Probabilities):\", action_probabilities)\n",
        "        print(\"Critic Output (State Value):\", value)\n",
        "    elif isinstance(env.action_space, gym.spaces.Box):\n",
        "        (mean, log_std), value = output\n",
        "        print(\"Actor Output (Mean):\", mean)\n",
        "        print(\"Actor Output (Log Std):\", log_std)\n",
        "        print(\"Critic Output (State Value):\", value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33EwxaA8rNb0",
        "outputId": "328f613f-7945-4edf-fce0-5f8e8db3581a"
      },
      "id": "33EwxaA8rNb0",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Creating network for CliffWalking-v0:\n",
            "Actor Output (Action Probabilities): tensor([[0.2469, 0.2396, 0.2684, 0.2451],\n",
            "        [0.2505, 0.2377, 0.2719, 0.2399],\n",
            "        [0.2515, 0.2371, 0.2715, 0.2399]], grad_fn=<SoftmaxBackward0>)\n",
            "Critic Output (State Value): tensor([[0.0211],\n",
            "        [0.0357],\n",
            "        [0.0219]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Creating network for LunarLander-v3:\n",
            "Actor Output (Action Probabilities): tensor([[0.2293, 0.2333, 0.3125, 0.2250],\n",
            "        [0.2376, 0.2296, 0.3096, 0.2232],\n",
            "        [0.2317, 0.2284, 0.2949, 0.2450]], grad_fn=<SoftmaxBackward0>)\n",
            "Critic Output (State Value): tensor([[0.0298],\n",
            "        [0.0094],\n",
            "        [0.0268]], grad_fn=<AddmmBackward0>)\n",
            " Failed to create environment PongNoFrameskip-v4: Environment `PongNoFrameskip` doesn't exist.\n",
            "\n",
            " Creating network for HalfCheetah-v5:\n",
            "Actor Output (Mean): tensor([[ 0.0841,  0.0689,  0.0924,  0.1195,  0.1081, -0.1889],\n",
            "        [ 0.2681,  0.2020,  0.0368,  0.0860,  0.1245, -0.1484],\n",
            "        [-0.0117,  0.0318,  0.0853,  0.2254, -0.0121, -0.0613]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Actor Output (Log Std): tensor([[0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0.]], grad_fn=<ExpandBackward0>)\n",
            "Critic Output (State Value): tensor([[0.0010],\n",
            "        [0.0380],\n",
            "        [0.1225]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "4ccd13f0b62b30ff"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "4ccd13f0b62b30ff"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first implementation presents a streamlined actorâ€“critic agent capable of handling both discrete and continuous action spaces with minimal ceremony. It employs a single hidden layer to transform the environmentâ€™s state into a shared feature vector, which then branches into two heads: one producing a scalar value estimate (the critic) and the other yielding action parameters (the actor). In discrete settings, the actor head emits logits that are converted to probabilities via softmax; in continuous domains, it predicts Gaussian means alongside a learned logâ€standardâ€deviation. By consolidating actor and critic into a single network and keeping the architecture shallow, this design minimizes code complexity and parameter count, making it particularly well suited for lowâ€‘dimensional, vectorâ€‘based tasks where rapid prototyping is paramount.\n",
        "\n",
        "A companion helper function automatically introspects the Gym environmentâ€™s observation and action spaces to infer input and output dimensions. Discrete observations are oneâ€‘hot encoded, Box observations are flattened into vectors, and action dimensions are read directly from the environmentâ€™s API. This metaâ€‘programming approach allows the same class and helper to instantiate agents for a variety of tasksâ€”from tabular CliffWalking to continuous control benchmarksâ€”without any manual reshaping or boilerplate. The printed dummy outputs serve as a sanity check, verifying that the networkâ€™s input and output shapes align with the environment, though they also underscore a limitation: flattening raw pixel inputs ignores spatial structure, which is critical for visionâ€‘based domains like Atari.\n",
        "\n",
        "The second implementation, built atop Gymnasium, extends representational capacity by adding a second shared hidden layer. This deeper backbone is better equipped to capture the nonlinear dynamics typical of more challenging environments, such as MuJoCoâ€™s HalfCheetahâ€‘v5. It retains the same automatic dimensionâ€‘inference logic, preserving the plugâ€‘andâ€‘play convenience of the first implementation. For continuous actions, a global logâ€‘standardâ€‘deviation parameter ensures stable yet tunable exploration, while discrete actions continue to leverage a softmax head over learned logits.\n",
        "\n",
        "In practical terms, the choice among these architectures depends on the complexity of the task and the computational budget available. For simple, lowâ€‘dimensional problemsâ€”where the state space can be effectively summarized by a shallow multilayer perceptronâ€”the singleâ€‘layer network offers unmatched simplicity and speed. When richer features are required to model complex dynamics, the twoâ€‘layer shared network strikes a favorable balance between expressivity and ease of use. And when dealing with highâ€‘dimensional visual inputs, practitioners are advised to replace the MLP encoder with convolutional or residual architectures to exploit spatial hierarchies, recognizing that deeper backbones are essential for extracting meaningful features from images."
      ],
      "metadata": {
        "id": "Vg4kf7YuZZ_D"
      },
      "id": "Vg4kf7YuZZ_D"
    },
    {
      "metadata": {
        "id": "b39c886fa536a639"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 3: Write Observation Normalization Function\n",
        "Create a function `normalize_observation(obs, env)` that:\n",
        "- Checks if the observation space is `Box` and has `low` and `high` attributes.\n",
        "- If so, normalize the input observation.\n",
        "- Otherwise, return the observation unchanged.\n",
        "\n",
        "```python\n",
        "# TODO: Define `normalize_observation(obs, env)`\n",
        "```\n",
        "\n",
        "Test this function with observations from:\n",
        "- `LunarLander-v3`\n",
        "- `PongNoFrameskip-v4`\n",
        "\n",
        "Note: Atari observations are image arrays. Normalize pixel values to [0, 1]. For LunarLander-v3, the different elements in the observation vector have different ranges. Normalize them to [0, 1] using the `low` and `high` attributes of the observation space.\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "b39c886fa536a639"
    },
    {
      "metadata": {
        "id": "fc7ee06112cf7d29"
      },
      "cell_type": "code",
      "source": [
        "import gymnasium as gymn\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "def normalize_observation(obs, env):\n",
        "    if isinstance(obs, dict):\n",
        "        return {k: normalize_observation(v, env) for k, v in obs.items()}\n",
        "    if isinstance(obs, (list, tuple)):\n",
        "        try:\n",
        "            arrays = [np.array(o) for o in obs]\n",
        "            shapes = [a.shape for a in arrays]\n",
        "            if len(set(shapes)) == 1:\n",
        "                obs = np.stack(arrays, axis=0)\n",
        "            else:\n",
        "                return type(obs)(normalize_observation(o, env) for o in obs)\n",
        "        except Exception:\n",
        "            return type(obs)(normalize_observation(o, env) for o in obs)\n",
        "    try:\n",
        "        obs = np.asarray(obs, dtype=np.float32)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Unable to convert observation to numpy array: {e}\")\n",
        "    if hasattr(env.observation_space, 'low') and hasattr(env.observation_space, 'high'):\n",
        "        if hasattr(env.observation_space, 'dtype') and env.observation_space.dtype == np.uint8:\n",
        "            normalized = obs / 255.0\n",
        "        else:\n",
        "            low = env.observation_space.low.astype(np.float32)\n",
        "            high = env.observation_space.high.astype(np.float32)\n",
        "            normalized = (obs - low) / (high - low)\n",
        "        normalized = np.clip(normalized, 0, 1)\n",
        "        return normalized\n",
        "    else:\n",
        "        return obs"
      ],
      "id": "fc7ee06112cf7d29",
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "env_lunar = gymn.make(\"LunarLander-v3\")\n",
        "obs_lunar, _ = env_lunar.reset()\n",
        "norm_obs_lunar = normalize_observation(obs_lunar, env_lunar)\n",
        "print(\"LunarLander-v3 original observation:\", obs_lunar)\n",
        "print(\"LunarLander-v3 normalized observation:\", norm_obs_lunar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkRTttmYtoiD",
        "outputId": "7049174a-0396-4b67-e1b9-d187966ecd4f"
      },
      "id": "BkRTttmYtoiD",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LunarLander-v3 original observation: [ 0.00670757  1.412558    0.6793884   0.07277823 -0.00776559 -0.1538915\n",
            "  0.          0.        ]\n",
            "LunarLander-v3 normalized observation: [0.5013415  0.7825116  0.5339694  0.5036389  0.49938202 0.49230543\n",
            " 0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_pong = gym.make(\"PongNoFrameskip-v4\")\n",
        "obs_pong = env_pong.reset()\n",
        "norm_obs_pong = normalize_observation(obs_pong, env_pong)\n",
        "if isinstance(norm_obs_pong, (list, tuple)):\n",
        "    shapes = [np.asarray(frame).shape for frame in norm_obs_pong]\n",
        "    mins = [np.min(frame) for frame in norm_obs_pong]\n",
        "    maxs = [np.max(frame) for frame in norm_obs_pong]\n",
        "    print(\"PongNoFrameskip-v4 normalized observation shapes:\", shapes)\n",
        "    print(\"PongNoFrameskip-v4 normalized observation value ranges:\")\n",
        "\n",
        "    for i, (mi, ma) in enumerate(zip(mins, maxs)):\n",
        "        print(f\"  Frame {i}: {mi} to {ma}\")\n",
        "\n",
        "else:\n",
        "    print(\"PongNoFrameskip-v4 original observation shape:\", np.asarray(obs_pong).shape)\n",
        "    print(\"PongNoFrameskip-v4 normalized observation range:\",\n",
        "          norm_obs_pong.min(), \"to\", norm_obs_pong.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwPrVVdEvpnR",
        "outputId": "f759bf60-f72a-48df-976f-c6bb1bed5220"
      },
      "id": "vwPrVVdEvpnR",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PongNoFrameskip-v4 normalized observation shapes: [(210, 160, 3), ()]\n",
            "PongNoFrameskip-v4 normalized observation value ranges:\n",
            "  Frame 0: 0.0 to 0.8941176533699036\n",
            "  Frame 1: {'lives': np.float32(0.0), 'episode_frame_number': np.float32(0.0), 'frame_number': np.float32(0.0)} to {'lives': np.float32(0.0), 'episode_frame_number': np.float32(0.0), 'frame_number': np.float32(0.0)}\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "501ed2a6e7ca7a7b"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "501ed2a6e7ca7a7b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The normalization function in the provided setup is designed with consistency and robustness in mind. Environments from Gym and Gymnasium often deliver observations in diverse formatsâ€”ranging from raw pixel arrays to structured dictionaries or flat numerical vectors. To handle this variability, the function recursively traverses through each component of the observation, converting elements into standardized NumPy arrays of float32 data type. This ensures that regardless of the structureâ€”be it a simple array or a nested dictionaryâ€”the output is uniform and compatible with downstream neural network inputs. As a result, model pipelines become more reliable and less prone to data-type-related errors.\n",
        "\n",
        "Once the observation is cast into a suitable numeric format, the function proceeds to normalize it within the\n",
        "0\n",
        ",\n",
        "\n",
        "1\n",
        "0,â€¯1 range. If the environment provides explicit low and high bounds through its observation space, a linear transformation is applied to each element, mapping the minimum to 0 and the maximum to 1. This step is critical for learning stability: without normalization, features with varying scales can produce erratic gradient updates, which may lead to unstable or slow training. By ensuring all features lie within the same range, the model benefits from smoother optimization dynamics and improved convergence behavior.\n",
        "\n",
        "In scenarios involving image-based observationsâ€”such as in Atari environments like Pongâ€”where the data is typically in uint8 format (ranging from 0 to 255), the function simply rescales pixel values by dividing by 255.0. This transformation, while simple, is a standard preprocessing step in computer vision tasks and enables agents to process visual data in a more numerically stable form. Though more advanced techniques like grayscale conversion or frame stacking may be used later in the pipeline, this basic normalization offers a practical baseline for quick prototyping and testing.\n",
        "\n",
        "Overall, this normalization strategy is especially useful during experimentation and early-stage development. It offers a generalized preprocessing approach that accommodates a wide variety of environments, from classic control tasks like LunarLander to more complex vision-based games like Pong. By abstracting away the intricacies of observation formats and scaling, researchers and practitioners can concentrate on refining learning algorithms without being encumbered by input inconsistencies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2AC-ftNhaTP-"
      },
      "id": "2AC-ftNhaTP-"
    },
    {
      "metadata": {
        "id": "6b5fb5353307f514"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 4: Gradient Clipping\n",
        "\n",
        "To prevent exploding gradients, it's common practice to clip gradients before optimizer updates.\n",
        "\n",
        "### Task 4: Clip Gradients for Actor-Critic Networks\n",
        "Use dummy tensors and apply gradient clipping with the following PyTorch method:\n",
        "```python\n",
        "# During training, after loss.backward():\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "```\n",
        "\n",
        "Reuse the loss computation from Task 1a or 1b. After computing the gradients, apply gradient clipping.\n",
        "Print the gradient norm before and after clipping to verify itâ€™s applied.\n",
        "\n",
        "ðŸ”— PyTorch Docs: https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "6b5fb5353307f514"
    },
    {
      "metadata": {
        "id": "7327507fb6e803ad"
      },
      "cell_type": "code",
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "\n",
        "class DummyActorCritic(nn.Module):\n",
        "    def __init__(self, input_size, num_actions):\n",
        "        super(DummyActorCritic, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, 128)\n",
        "        self.actor = nn.Linear(128, num_actions)\n",
        "        self.critic = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc(x))\n",
        "        action_probabilities = F.softmax(self.actor(x), dim=-1)\n",
        "        value = self.critic(x)\n",
        "        return action_probabilities, value\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "7327507fb6e803ad",
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_training_with_grad_clip(model, state):\n",
        "    action_probabilities, value = model(state)\n",
        "    dummy_log_probabilities = torch.log(action_probabilities + 1e-8)\n",
        "    dummy_return_values = torch.randn_like(value)\n",
        "    dummy_entropies = -torch.sum(action_probabilities * dummy_log_probabilities, dim=1, keepdim=True)\n",
        "    advantage = dummy_return_values - value.detach()\n",
        "    actor_loss = -(dummy_log_probabilities * advantage).mean() - 0.01 * dummy_entropies.mean()\n",
        "    critic_loss = F.mse_loss(value, dummy_return_values)\n",
        "    total_loss = actor_loss + critic_loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    pre_clip_norm = 0.0\n",
        "\n",
        "    for p in model.parameters():\n",
        "        if p.grad is not None:\n",
        "            pre_clip_norm += p.grad.data.norm(2).item() ** 2\n",
        "\n",
        "    pre_clip_norm = pre_clip_norm ** 0.5\n",
        "    clip_norm = utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "    post_clip_norm = 0.0\n",
        "\n",
        "    for p in model.parameters():\n",
        "        if p.grad is not None:\n",
        "            post_clip_norm += p.grad.data.norm(2).item() ** 2\n",
        "\n",
        "    post_clip_norm = post_clip_norm ** 0.5\n",
        "    optimizer.step()\n",
        "    print(\"Single Optimizer Training Step:\")\n",
        "    print(\"Actor Loss:\", actor_loss.item())\n",
        "    print(\"Critic Loss:\", critic_loss.item())\n",
        "    print(\"Total Loss:\", total_loss.item())\n",
        "    print(\"Gradient norm before clipping:\", pre_clip_norm)\n",
        "    print(\"Gradient norm reported by clip_grad_norm_ (before clipping):\", clip_norm)\n",
        "    print(\"Gradient norm after clipping:\", post_clip_norm)"
      ],
      "metadata": {
        "id": "1ZtjO1_1xSx9"
      },
      "id": "1ZtjO1_1xSx9",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 8\n",
        "num_actions = 4\n",
        "model = DummyActorCritic(input_size, num_actions)\n",
        "state = torch.randn(1, input_size)\n",
        "simulate_training_with_grad_clip(model, state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtSAd_w-xVb5",
        "outputId": "2123a560-a64c-4e30-8225-b1a20ebf672d"
      },
      "id": "RtSAd_w-xVb5",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Optimizer Training Step:\n",
            "Actor Loss: -2.9831323623657227\n",
            "Critic Loss: 4.527118682861328\n",
            "Total Loss: 1.5439863204956055\n",
            "Gradient norm before clipping: 23.084273351243823\n",
            "Gradient norm reported by clip_grad_norm_ (before clipping): tensor(23.0843)\n",
            "Gradient norm after clipping: 0.4999999788338938\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "9952750fa74cd487"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:"
      ],
      "id": "9952750fa74cd487"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In highâ€‘variance settingsâ€”particularly deep or recurrent architectures used in reinforcement learningâ€”it is not uncommon for gradient magnitudes to spike unexpectedly, destabilizing training or even producing NaNs. Gradient clipping addresses this by capping the global â„“â‚‚ norm of all gradients (0.5 in the provided example), ensuring that no single update can become excessively large. This safeguard allows practitioners to maintain an aggressive learning rate without fearing that an isolated gradient explosion will derail the entire training process.\n",
        "\n",
        "The DummyActorCritic architecture exemplifies a widely adopted design in actorâ€“critic methods: a shared â€œbackboneâ€ network that projects raw observations into a 128â€‘dimensional feature space, which then bifurcates into two distinct headsâ€”one estimating the policy and the other estimating state value. Sharing these lowerâ€‘level parameters not only reduces the overall model size but also fosters beneficial inductive biases, since the criticâ€™s insights about state quality can inform the actorâ€™s action preferences, and vice versa. Empirically, this parameter sharing often accelerates convergence and stabilizes learning.\n",
        "\n",
        "For discrete action domains, the actor headâ€™s raw outputs are converted into a valid probability distribution via softmax. The core policyâ€‘gradient objectiveâ€”the negative expected logâ€‘probability of chosen actions weighted by the estimated advantageâ€”drives the agent toward actions yielding higher longâ€‘term returns. To prevent premature convergence to a narrow set of actions, a modest entropy bonus is incorporated into the loss. This term rewards uncertainty, encouraging the policy to continue exploring until sufficient evidence accumulates for confident decisionâ€‘making.\n",
        "\n",
        "Rather than managing separate optimizers or learningâ€‘rate schedules for the actor and critic, the example consolidates all parameters under a single Adam optimizer. This unified approach simplifies hyperparameter tuning and implicitly couples the learning rates of both heads. During early experimentation or prototypingâ€”when actor and critic losses typically remain on comparable scalesâ€”this simplicity often outweighs the benefits of decoupling. Only when one loss term begins to dominate does the case for multiple optimizers become compelling.\n",
        "\n",
        "Before integrating with a real environment or computing actual Monte Carlo returns, the pipeline undergoes a â€œsmoke testâ€ using randomly generated dummy returns. By executing a full forward pass, computing both actor and critic losses, applying gradient clipping, and stepping the optimizer on synthetic data, this initial check verifies that tensor dimensions, gradient flows, clipping logic, and optimizer calls are all implemented correctly. Identifying and resolving these foundational issues on noise alone is far more efficient than debugging them amidst the complexity of live rollâ€‘outs."
      ],
      "metadata": {
        "id": "Lr5R3wjqbHMd"
      },
      "id": "Lr5R3wjqbHMd"
    },
    {
      "metadata": {
        "id": "f4cff31e6c6e7e4a"
      },
      "cell_type": "markdown",
      "source": [
        "If you are working in a team, provide a contribution summary.\n",
        "| Team Member | Step# | Contribution (%) |\n",
        "|---|---|---|\n",
        "|   | Task 1 |   |\n",
        "|   | Task 2 |   |\n",
        "|   | Task 3 |   |\n",
        "|   | Task 4 |   |\n",
        "|   | **Total** |   |\n"
      ],
      "id": "f4cff31e6c6e7e4a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shishir Hebbar - Task 1,2,3,4 - 50% <br>\n",
        "Shaurya Mathur - Task 1,2,3,4 - 50%"
      ],
      "metadata": {
        "id": "at2tZN7GbWuS"
      },
      "id": "at2tZN7GbWuS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Refrences:"
      ],
      "metadata": {
        "id": "tnZeugHYb1F3"
      },
      "id": "tnZeugHYb1F3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) https://www.geeksforgeeks.org/actor-critic-algorithm-in-reinforcement-learning/ <br>\n",
        "2) https://www.gymlibrary.dev/environments/toy_text/cliff_walking/ <br>\n",
        "3) https://www.gymlibrary.dev/environments/box2d/lunar_lander/ <br>\n",
        "4) https://huggingface.co/ThomasSimonini/ppo-PongNoFrameskip-v4 <br>\n",
        "5) https://www.gymlibrary.dev/environments/mujoco/half_cheetah/"
      ],
      "metadata": {
        "id": "V265EIpQb37L"
      },
      "id": "V265EIpQb37L"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}